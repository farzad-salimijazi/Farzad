{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCI566_Intro_PyTorch (coding).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farzad-salimijazi/Farzad/blob/master/CSCI566_Intro_PyTorch_(coding).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE0tpgUDQCA2",
        "colab_type": "text"
      },
      "source": [
        "# CSCI566 Lecture 14. PyTorch and Deep Reinforcement Learning Implementation\n",
        "\n",
        "\n",
        "This notebook is for introducing basic usage of PyTorch and an example implementation of deep reinforcement learning algorithm (simple q-learning).\n",
        "\n",
        "\n",
        "It covers introduction to\n",
        "\n",
        "*   PyTorch\n",
        "*   OpenAI gym\n",
        "*   Q-learning implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_jO0H60RiSe",
        "colab_type": "text"
      },
      "source": [
        "## PyTorch\n",
        "\n",
        "You can find a tutorial at https://bit.ly/60minblitz .  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NmH0ndrnipE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorch \n",
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spP8Fvaaamrz",
        "colab_type": "text"
      },
      "source": [
        "### Tensors\n",
        "\n",
        "PyTorch's tensors are similar to numpy's ndarrays. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6doUZOQRl7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1cf7db42-2786-4176-ddef-33680995b06c"
      },
      "source": [
        "# Zero tensor\n",
        "z = torch.zeros(2,3)\n",
        "\n",
        "print('zeros: ', z)\n",
        "print('dtype: ', z.dtype)\n",
        "print('shape: ', z.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "zeros:  tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "dtype:  torch.float32\n",
            "shape:  torch.Size([2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpExm4PioQux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4bddd4ce-0426-4656-dc78-855c3d30934f"
      },
      "source": [
        "# One tensor\n",
        "o = torch.ones(2,3)\n",
        "\n",
        "print('ones: ', o)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ones:  tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXQM8WKwoR75",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "612b0409-70e5-4538-a57b-aa3b3506bf6b"
      },
      "source": [
        "# Random tensor\n",
        "r = torch.randn(2,3)\n",
        "\n",
        "print('random: ', r)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random:  tensor([[ 0.8456,  0.9450, -0.7572],\n",
            "        [-1.1304,  0.9331, -0.5778]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AJOig6UoR1g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9d3f3566-b636-4b39-b930-8d6e19ed5204"
      },
      "source": [
        "# Identity tensor\n",
        "i = torch.eye(3)\n",
        "\n",
        "print('identity: ', i)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "identity:  tensor([[1., 0., 0.],\n",
            "        [0., 1., 0.],\n",
            "        [0., 0., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0TWGTokoRuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c03158e1-c142-48c3-966a-50872048e1be"
      },
      "source": [
        "# Tensor from values, numpy array, list\n",
        "a_scalar = 10.10\n",
        "a = torch.tensor(a_scalar)\n",
        "\n",
        "print(a)\n",
        "\n",
        "b_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "b = torch.tensor(b_array)\n",
        "\n",
        "print(b) \n",
        "\n",
        "x_list = [5., 6., 6.]\n",
        "x = torch.tensor(x_list)\n",
        "\n",
        "print(x)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(10.1000)\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([5., 6., 6.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rs6pVroRl4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7fcdcadc-82ae-42cc-b5a7-8efffeb9b8b7"
      },
      "source": [
        "# Tensor with autograd\n",
        "v = torch.tensor(x_list, requires_grad=True)\n",
        "\n",
        "print(v)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5., 6., 6.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPQl23yToRVM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a97ead9b-5694-4b3c-ab83-39511e2b1ca1"
      },
      "source": [
        "# Tensor to scalar\n",
        "print(a.item())\n",
        "\n",
        "\n",
        "# Tensor to numpy array\n",
        "print(x.numpy())\n",
        "\n",
        "\n",
        "#print(v.numpy()) # will generate an error\n",
        "\n",
        "# Tensor with gradient needs to be detached before numpy()\n",
        "print(v.detach().numpy())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.100000381469727\n",
            "[5. 6. 6.]\n",
            "[5. 6. 6.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IalZ8BEygS8c",
        "colab_type": "text"
      },
      "source": [
        "### Operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnZu1wcRR18o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "82c59f08-cde3-41bc-edce-36dc52988553"
      },
      "source": [
        "# Add\n",
        "print(o + r)\n",
        "\n",
        "# Element-wise multiplication\n",
        "print(r * o)\n",
        "\n",
        "# Matrix multiplication\n",
        "print(torch.matmul(r, i))\n",
        "\n",
        "# Reshape\n",
        "print(o.view(3, 2, 1)) # only change view if possible , you do not manipulate it but indexes are in a new view\n",
        "print(o.reshape(3, 2)) # always rearrange values in memory"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.8456,  1.9450,  0.2428],\n",
            "        [-0.1304,  1.9331,  0.4222]])\n",
            "tensor([[ 0.8456,  0.9450, -0.7572],\n",
            "        [-1.1304,  0.9331, -0.5778]])\n",
            "tensor([[ 0.8456,  0.9450, -0.7572],\n",
            "        [-1.1304,  0.9331, -0.5778]])\n",
            "tensor([[[1.],\n",
            "         [1.]],\n",
            "\n",
            "        [[1.],\n",
            "         [1.]],\n",
            "\n",
            "        [[1.],\n",
            "         [1.]]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qz2BcAAjKuU",
        "colab_type": "text"
      },
      "source": [
        "### Gradient\n",
        "The code below shows a simple example of autograd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDG6oP-QjMo3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "599680a8-238e-48a1-d6f6-7dfcded8167b"
      },
      "source": [
        "# y = (x - 1)^2 + 2\n",
        "\n",
        "x = torch.randn(1, requires_grad=True)\n",
        "y = (x - 1).pow(2) + 2\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.1220], requires_grad=True)\n",
            "tensor([2.0149], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mnkPh-fkQIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0c0614d-bd66-40c3-c244-3a1f16be4ec2"
      },
      "source": [
        "# Find the minimum of y\n",
        "for _ in range(1000):\n",
        "  y = (x - 1).pow(2) + 2\n",
        "  y.backward()\n",
        "  \n",
        "  x.data = x.data - 0.01*x.grad.data\n",
        "  print(y.item(), x.grad.data.item())\n",
        "  x.grad.data.zero_()\n",
        "\n",
        "print('y is minimum at x = 1')\n",
        "print('x = ', x.item())\n",
        "print('y = ', y.item())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0148744583129883 0.24392175674438477\n",
            "2.0142853260040283 0.2390432357788086\n",
            "2.0137197971343994 0.23426246643066406\n",
            "2.013176441192627 0.22957730293273926\n",
            "2.0126545429229736 0.22498583793640137\n",
            "2.0121536254882812 0.22048616409301758\n",
            "2.011672258377075 0.21607637405395508\n",
            "2.0112099647521973 0.21175479888916016\n",
            "2.01076602935791 0.2075197696685791\n",
            "2.0103397369384766 0.2033693790435791\n",
            "2.009930372238159 0.19930195808410645\n",
            "2.0095369815826416 0.19531583786010742\n",
            "2.0091593265533447 0.19140958786010742\n",
            "2.0087966918945312 0.18758130073547363\n",
            "2.008448362350464 0.18382978439331055\n",
            "2.0081138610839844 0.18015313148498535\n",
            "2.0077924728393555 0.17655014991760254\n",
            "2.007483959197998 0.1730191707611084\n",
            "2.007187604904175 0.16955876350402832\n",
            "2.0069029331207275 0.1661674976348877\n",
            "2.006629467010498 0.16284418106079102\n",
            "2.0063669681549072 0.15958738327026367\n",
            "2.006114959716797 0.15639567375183105\n",
            "2.0058727264404297 0.15326786041259766\n",
            "2.0056402683258057 0.15020251274108887\n",
            "2.0054168701171875 0.14719843864440918\n",
            "2.005202293395996 0.14425444602966309\n",
            "2.0049962997436523 0.14136934280395508\n",
            "2.004798412322998 0.13854193687438965\n",
            "2.004608392715454 0.1357710361480713\n",
            "2.0044260025024414 0.1330556869506836\n",
            "2.0042507648468018 0.13039445877075195\n",
            "2.004082441329956 0.12778663635253906\n",
            "2.003920793533325 0.1252307891845703\n",
            "2.003765344619751 0.1227262020111084\n",
            "2.0036163330078125 0.12027168273925781\n",
            "2.0034730434417725 0.11786627769470215\n",
            "2.003335475921631 0.115509033203125\n",
            "2.0032033920288086 0.11319875717163086\n",
            "2.0030765533447266 0.11093473434448242\n",
            "2.0029547214508057 0.10871601104736328\n",
            "2.002837896347046 0.10654163360595703\n",
            "2.00272536277771 0.10441088676452637\n",
            "2.002617359161377 0.10232257843017578\n",
            "2.002513885498047 0.10027623176574707\n",
            "2.0024142265319824 0.09827065467834473\n",
            "2.0023186206817627 0.09630513191223145\n",
            "2.0022268295288086 0.09437894821166992\n",
            "2.002138614654541 0.09249138832092285\n",
            "2.00205397605896 0.09064149856567383\n",
            "2.0019726753234863 0.08882856369018555\n",
            "2.001894474029541 0.0870521068572998\n",
            "2.001819610595703 0.0853111743927002\n",
            "2.0017473697662354 0.08360505104064941\n",
            "2.001678228378296 0.08193302154541016\n",
            "2.0016117095947266 0.08029437065124512\n",
            "2.0015480518341064 0.07868838310241699\n",
            "2.0014867782592773 0.07711458206176758\n",
            "2.0014278888702393 0.07557225227355957\n",
            "2.001371145248413 0.07406091690063477\n",
            "2.001317024230957 0.07257962226867676\n",
            "2.001264810562134 0.07112812995910645\n",
            "2.0012147426605225 0.06970548629760742\n",
            "2.001166582107544 0.06831145286560059\n",
            "2.0011203289031982 0.06694531440734863\n",
            "2.0010759830474854 0.06560635566711426\n",
            "2.0010335445404053 0.06429433822631836\n",
            "2.0009925365448 0.06300854682922363\n",
            "2.000953197479248 0.06174826622009277\n",
            "2.00091552734375 0.06051325798034668\n",
            "2.0008792877197266 0.05930304527282715\n",
            "2.0008444786071777 0.058116912841796875\n",
            "2.0008108615875244 0.05695462226867676\n",
            "2.000778913497925 0.05581545829772949\n",
            "2.0007479190826416 0.05469918251037598\n",
            "2.000718355178833 0.053605079650878906\n",
            "2.00068998336792 0.05253291130065918\n",
            "2.0006625652313232 0.051482200622558594\n",
            "2.000636339187622 0.050452470779418945\n",
            "2.0006110668182373 0.04944348335266113\n",
            "2.000586986541748 0.04845452308654785\n",
            "2.000563621520996 0.0474853515625\n",
            "2.0005414485931396 0.04653573036193848\n",
            "2.0005199909210205 0.04560494422912598\n",
            "2.0004992485046387 0.0446927547454834\n",
            "2.0004796981811523 0.04379892349243164\n",
            "2.000460624694824 0.0429229736328125\n",
            "2.0004422664642334 0.04206442832946777\n",
            "2.000424861907959 0.04122304916381836\n",
            "2.0004079341888428 0.040398597717285156\n",
            "2.000391960144043 0.03959059715270996\n",
            "2.0003762245178223 0.03879880905151367\n",
            "2.000361442565918 0.038022756576538086\n",
            "2.000347137451172 0.0372622013092041\n",
            "2.000333309173584 0.03651690483093262\n",
            "2.0003201961517334 0.03578662872314453\n",
            "2.000307559967041 0.03507089614868164\n",
            "2.000295400619507 0.034369468688964844\n",
            "2.000283718109131 0.03368210792541504\n",
            "2.000272274017334 0.033008575439453125\n",
            "2.0002615451812744 0.0323483943939209\n",
            "2.000251293182373 0.03170132637023926\n",
            "2.000241279602051 0.031067371368408203\n",
            "2.0002317428588867 0.03044605255126953\n",
            "2.0002224445343018 0.02983713150024414\n",
            "2.000213861465454 0.02924036979675293\n",
            "2.0002052783966064 0.028655529022216797\n",
            "2.000197172164917 0.02808237075805664\n",
            "2.0001893043518066 0.02752065658569336\n",
            "2.0001819133758545 0.02697014808654785\n",
            "2.0001747608184814 0.026430845260620117\n",
            "2.0001678466796875 0.025902271270751953\n",
            "2.0001611709594727 0.025384187698364258\n",
            "2.000154733657837 0.02487659454345703\n",
            "2.0001485347747803 0.02437901496887207\n",
            "2.000142812728882 0.023891448974609375\n",
            "2.0001370906829834 0.023413658142089844\n",
            "2.000131607055664 0.022945404052734375\n",
            "2.000126361846924 0.022486448287963867\n",
            "2.0001213550567627 0.02203679084777832\n",
            "2.0001165866851807 0.02159595489501953\n",
            "2.0001120567321777 0.0211639404296875\n",
            "2.000107526779175 0.020740747451782227\n",
            "2.000103235244751 0.020325899124145508\n",
            "2.0000991821289062 0.019919395446777344\n",
            "2.0000953674316406 0.019520998001098633\n",
            "2.000091552734375 0.019130468368530273\n",
            "2.0000879764556885 0.018747806549072266\n",
            "2.000084400177002 0.018372774124145508\n",
            "2.0000810623168945 0.01800537109375\n",
            "2.000077724456787 0.01764535903930664\n",
            "2.000074863433838 0.017292499542236328\n",
            "2.0000717639923096 0.01694655418395996\n",
            "2.0000689029693604 0.01660752296447754\n",
            "2.0000662803649902 0.016275405883789062\n",
            "2.00006365776062 0.01594996452331543\n",
            "2.00006103515625 0.01563096046447754\n",
            "2.000058650970459 0.01531839370727539\n",
            "2.000056266784668 0.015012025833129883\n",
            "2.000054121017456 0.014711856842041016\n",
            "2.000051975250244 0.014417648315429688\n",
            "2.0000498294830322 0.014129400253295898\n",
            "2.0000479221343994 0.013846874237060547\n",
            "2.0000460147857666 0.013569831848144531\n",
            "2.000044107437134 0.013298511505126953\n",
            "2.00004243850708 0.01303243637084961\n",
            "2.0000407695770264 0.012771844863891602\n",
            "2.0000391006469727 0.012516498565673828\n",
            "2.000037670135498 0.012266159057617188\n",
            "2.0000362396240234 0.01202082633972168\n",
            "2.000034809112549 0.011780500411987305\n",
            "2.000033378601074 0.011544942855834961\n",
            "2.0000319480895996 0.011314153671264648\n",
            "2.000030755996704 0.011087894439697266\n",
            "2.0000295639038086 0.010866165161132812\n",
            "2.000028371810913 0.010648727416992188\n",
            "2.0000271797180176 0.010435819625854492\n",
            "2.000026226043701 0.010227203369140625\n",
            "2.0000250339508057 0.010022640228271484\n",
            "2.0000240802764893 0.00982213020324707\n",
            "2.000023126602173 0.009625673294067383\n",
            "2.0000221729278564 0.009433269500732422\n",
            "2.000021457672119 0.009244680404663086\n",
            "2.0000205039978027 0.009059906005859375\n",
            "2.0000197887420654 0.008878707885742188\n",
            "2.000018835067749 0.008701086044311523\n",
            "2.0000181198120117 0.008527040481567383\n",
            "2.0000174045562744 0.008356571197509766\n",
            "2.000016689300537 0.00818943977355957\n",
            "2.000016212463379 0.008025646209716797\n",
            "2.0000154972076416 0.007865190505981445\n",
            "2.0000147819519043 0.007707834243774414\n",
            "2.000014305114746 0.007553577423095703\n",
            "2.000013589859009 0.0074024200439453125\n",
            "2.0000131130218506 0.007254362106323242\n",
            "2.0000126361846924 0.007109165191650391\n",
            "2.000012159347534 0.006967067718505859\n",
            "2.000011682510376 0.006827831268310547\n",
            "2.0000112056732178 0.0066912174224853516\n",
            "2.0000107288360596 0.006557464599609375\n",
            "2.0000102519989014 0.006426334381103516\n",
            "2.0000100135803223 0.0062978267669677734\n",
            "2.000009536743164 0.0061719417572021484\n",
            "2.000009059906006 0.006048440933227539\n",
            "2.0000088214874268 0.005927562713623047\n",
            "2.0000083446502686 0.00580906867980957\n",
            "2.0000081062316895 0.005692958831787109\n",
            "2.0000078678131104 0.0055789947509765625\n",
            "2.000007390975952 0.005467414855957031\n",
            "2.000007152557373 0.005357980728149414\n",
            "2.000006914138794 0.0052509307861328125\n",
            "2.000006675720215 0.005146026611328125\n",
            "2.0000064373016357 0.00504302978515625\n",
            "2.0000061988830566 0.004942178726196289\n",
            "2.0000059604644775 0.004843235015869141\n",
            "2.0000057220458984 0.004746437072753906\n",
            "2.0000054836273193 0.004651546478271484\n",
            "2.0000052452087402 0.004558563232421875\n",
            "2.000005006790161 0.004467487335205078\n",
            "2.000004768371582 0.004378080368041992\n",
            "2.000004529953003 0.004290580749511719\n",
            "2.000004529953003 0.004204750061035156\n",
            "2.000004291534424 0.004120588302612305\n",
            "2.0000040531158447 0.004038095474243164\n",
            "2.0000038146972656 0.003957271575927734\n",
            "2.0000038146972656 0.0038781166076660156\n",
            "2.0000035762786865 0.003800630569458008\n",
            "2.0000035762786865 0.0037245750427246094\n",
            "2.0000033378601074 0.003650188446044922\n",
            "2.0000030994415283 0.0035772323608398438\n",
            "2.0000030994415283 0.003505706787109375\n",
            "2.000002861022949 0.0034356117248535156\n",
            "2.000002861022949 0.0033669471740722656\n",
            "2.00000262260437 0.003299713134765625\n",
            "2.00000262260437 0.003233671188354492\n",
            "2.00000262260437 0.0031690597534179688\n",
            "2.000002384185791 0.003105640411376953\n",
            "2.000002384185791 0.0030434131622314453\n",
            "2.000002145767212 0.002982616424560547\n",
            "2.000002145767212 0.0029230117797851562\n",
            "2.000002145767212 0.0028645992279052734\n",
            "2.000001907348633 0.0028073787689208984\n",
            "2.000001907348633 0.0027513504028320312\n",
            "2.000001907348633 0.0026962757110595703\n",
            "2.0000016689300537 0.002642393112182617\n",
            "2.0000016689300537 0.0025894641876220703\n",
            "2.0000016689300537 0.0025377273559570312\n",
            "2.0000014305114746 0.0024869441986083984\n",
            "2.0000014305114746 0.002437114715576172\n",
            "2.0000014305114746 0.002388477325439453\n",
            "2.0000014305114746 0.0023407936096191406\n",
            "2.0000014305114746 0.0022940635681152344\n",
            "2.0000011920928955 0.0022482872009277344\n",
            "2.0000011920928955 0.002203226089477539\n",
            "2.0000011920928955 0.00215911865234375\n",
            "2.0000011920928955 0.002115964889526367\n",
            "2.0000011920928955 0.0020737648010253906\n",
            "2.0000009536743164 0.0020322799682617188\n",
            "2.0000009536743164 0.001991748809814453\n",
            "2.0000009536743164 0.0019519329071044922\n",
            "2.0000009536743164 0.001912832260131836\n",
            "2.0000009536743164 0.001874685287475586\n",
            "2.0000009536743164 0.0018372535705566406\n",
            "2.0000007152557373 0.001800537109375\n",
            "2.0000007152557373 0.001764535903930664\n",
            "2.0000007152557373 0.0017292499542236328\n",
            "2.0000007152557373 0.0016946792602539062\n",
            "2.0000007152557373 0.0016608238220214844\n",
            "2.0000007152557373 0.0016276836395263672\n",
            "2.0000007152557373 0.0015950202941894531\n",
            "2.0000007152557373 0.0015630722045898438\n",
            "2.000000476837158 0.001531839370727539\n",
            "2.000000476837158 0.0015010833740234375\n",
            "2.000000476837158 0.0014710426330566406\n",
            "2.000000476837158 0.0014417171478271484\n",
            "2.000000476837158 0.0014128684997558594\n",
            "2.000000476837158 0.0013844966888427734\n",
            "2.000000476837158 0.0013568401336669922\n",
            "2.000000476837158 0.001329660415649414\n",
            "2.000000476837158 0.001302957534790039\n",
            "2.000000476837158 0.0012769699096679688\n",
            "2.000000476837158 0.0012514591217041016\n",
            "2.000000476837158 0.0012264251708984375\n",
            "2.000000476837158 0.0012018680572509766\n",
            "2.000000238418579 0.0011777877807617188\n",
            "2.000000238418579 0.001154184341430664\n",
            "2.000000238418579 0.0011310577392578125\n",
            "2.000000238418579 0.001108407974243164\n",
            "2.000000238418579 0.0010862350463867188\n",
            "2.000000238418579 0.0010645389556884766\n",
            "2.000000238418579 0.0010433197021484375\n",
            "2.000000238418579 0.0010223388671875\n",
            "2.000000238418579 0.0010018348693847656\n",
            "2.000000238418579 0.0009818077087402344\n",
            "2.000000238418579 0.0009622573852539062\n",
            "2.000000238418579 0.0009429454803466797\n",
            "2.000000238418579 0.0009241104125976562\n",
            "2.000000238418579 0.0009055137634277344\n",
            "2.000000238418579 0.0008873939514160156\n",
            "2.000000238418579 0.0008697509765625\n",
            "2.000000238418579 0.0008523464202880859\n",
            "2.000000238418579 0.000835418701171875\n",
            "2.000000238418579 0.0008187294006347656\n",
            "2.000000238418579 0.0008022785186767578\n",
            "2.000000238418579 0.0007863044738769531\n",
            "2.000000238418579 0.00077056884765625\n",
            "2.000000238418579 0.0007550716400146484\n",
            "2.000000238418579 0.00074005126953125\n",
            "2.000000238418579 0.0007252693176269531\n",
            "2.000000238418579 0.0007107257843017578\n",
            "2.000000238418579 0.0006964206695556641\n",
            "2.0 0.0006825923919677734\n",
            "2.0 0.0006690025329589844\n",
            "2.0 0.0006556510925292969\n",
            "2.0 0.0006425380706787109\n",
            "2.0 0.0006296634674072266\n",
            "2.0 0.0006170272827148438\n",
            "2.0 0.0006046295166015625\n",
            "2.0 0.0005924701690673828\n",
            "2.0 0.0005805492401123047\n",
            "2.0 0.0005688667297363281\n",
            "2.0 0.0005574226379394531\n",
            "2.0 0.0005462169647216797\n",
            "2.0 0.0005352497100830078\n",
            "2.0 0.0005245208740234375\n",
            "2.0 0.0005140304565429688\n",
            "2.0 0.0005037784576416016\n",
            "2.0 0.0004937648773193359\n",
            "2.0 0.0004839897155761719\n",
            "2.0 0.0004742145538330078\n",
            "2.0 0.0004646778106689453\n",
            "2.0 0.0004553794860839844\n",
            "2.0 0.000446319580078125\n",
            "2.0 0.0004374980926513672\n",
            "2.0 0.0004286766052246094\n",
            "2.0 0.0004200935363769531\n",
            "2.0 0.00041174888610839844\n",
            "2.0 0.00040340423583984375\n",
            "2.0 0.0003952980041503906\n",
            "2.0 0.00038743019104003906\n",
            "2.0 0.0003795623779296875\n",
            "2.0 0.0003719329833984375\n",
            "2.0 0.00036454200744628906\n",
            "2.0 0.0003571510314941406\n",
            "2.0 0.00034999847412109375\n",
            "2.0 0.00034308433532714844\n",
            "2.0 0.0003361701965332031\n",
            "2.0 0.0003294944763183594\n",
            "2.0 0.0003228187561035156\n",
            "2.0 0.00031638145446777344\n",
            "2.0 0.00030994415283203125\n",
            "2.0 0.0003037452697753906\n",
            "2.0 0.00029778480529785156\n",
            "2.0 0.0002918243408203125\n",
            "2.0 0.000286102294921875\n",
            "2.0 0.0002803802490234375\n",
            "2.0 0.000274658203125\n",
            "2.0 0.00026917457580566406\n",
            "2.0 0.0002636909484863281\n",
            "2.0 0.00025844573974609375\n",
            "2.0 0.0002532005310058594\n",
            "2.0 0.00024819374084472656\n",
            "2.0 0.00024318695068359375\n",
            "2.0 0.0002384185791015625\n",
            "2.0 0.00023365020751953125\n",
            "2.0 0.0002288818359375\n",
            "2.0 0.0002243518829345703\n",
            "2.0 0.00021982192993164062\n",
            "2.0 0.0002155303955078125\n",
            "2.0 0.00021123886108398438\n",
            "2.0 0.00020694732666015625\n",
            "2.0 0.0002028942108154297\n",
            "2.0 0.00019884109497070312\n",
            "2.0 0.00019478797912597656\n",
            "2.0 0.00019097328186035156\n",
            "2.0 0.00018715858459472656\n",
            "2.0 0.00018334388732910156\n",
            "2.0 0.00017976760864257812\n",
            "2.0 0.0001761913299560547\n",
            "2.0 0.00017261505126953125\n",
            "2.0 0.00016927719116210938\n",
            "2.0 0.0001659393310546875\n",
            "2.0 0.00016260147094726562\n",
            "2.0 0.00015926361083984375\n",
            "2.0 0.00015616416931152344\n",
            "2.0 0.00015306472778320312\n",
            "2.0 0.0001499652862548828\n",
            "2.0 0.0001468658447265625\n",
            "2.0 0.00014400482177734375\n",
            "2.0 0.000141143798828125\n",
            "2.0 0.00013828277587890625\n",
            "2.0 0.0001354217529296875\n",
            "2.0 0.0001327991485595703\n",
            "2.0 0.00013017654418945312\n",
            "2.0 0.00012755393981933594\n",
            "2.0 0.00012493133544921875\n",
            "2.0 0.00012254714965820312\n",
            "2.0 0.0001201629638671875\n",
            "2.0 0.00011777877807617188\n",
            "2.0 0.00011539459228515625\n",
            "2.0 0.00011301040649414062\n",
            "2.0 0.00011086463928222656\n",
            "2.0 0.0001087188720703125\n",
            "2.0 0.00010657310485839844\n",
            "2.0 0.00010442733764648438\n",
            "2.0 0.00010228157043457031\n",
            "2.0 0.00010013580322265625\n",
            "2.0 9.822845458984375e-05\n",
            "2.0 9.632110595703125e-05\n",
            "2.0 9.441375732421875e-05\n",
            "2.0 9.250640869140625e-05\n",
            "2.0 9.059906005859375e-05\n",
            "2.0 8.869171142578125e-05\n",
            "2.0 8.702278137207031e-05\n",
            "2.0 8.535385131835938e-05\n",
            "2.0 8.368492126464844e-05\n",
            "2.0 8.20159912109375e-05\n",
            "2.0 8.034706115722656e-05\n",
            "2.0 7.867813110351562e-05\n",
            "2.0 7.700920104980469e-05\n",
            "2.0 7.557868957519531e-05\n",
            "2.0 7.414817810058594e-05\n",
            "2.0 7.271766662597656e-05\n",
            "2.0 7.128715515136719e-05\n",
            "2.0 6.985664367675781e-05\n",
            "2.0 6.842613220214844e-05\n",
            "2.0 6.699562072753906e-05\n",
            "2.0 6.556510925292969e-05\n",
            "2.0 6.437301635742188e-05\n",
            "2.0 6.318092346191406e-05\n",
            "2.0 6.198883056640625e-05\n",
            "2.0 6.079673767089844e-05\n",
            "2.0 5.9604644775390625e-05\n",
            "2.0 5.841255187988281e-05\n",
            "2.0 5.7220458984375e-05\n",
            "2.0 5.602836608886719e-05\n",
            "2.0 5.4836273193359375e-05\n",
            "2.0 5.364418029785156e-05\n",
            "2.0 5.245208740234375e-05\n",
            "2.0 5.14984130859375e-05\n",
            "2.0 5.054473876953125e-05\n",
            "2.0 4.9591064453125e-05\n",
            "2.0 4.863739013671875e-05\n",
            "2.0 4.76837158203125e-05\n",
            "2.0 4.673004150390625e-05\n",
            "2.0 4.57763671875e-05\n",
            "2.0 4.482269287109375e-05\n",
            "2.0 4.38690185546875e-05\n",
            "2.0 4.291534423828125e-05\n",
            "2.0 4.1961669921875e-05\n",
            "2.0 4.100799560546875e-05\n",
            "2.0 4.029273986816406e-05\n",
            "2.0 3.9577484130859375e-05\n",
            "2.0 3.886222839355469e-05\n",
            "2.0 3.814697265625e-05\n",
            "2.0 3.743171691894531e-05\n",
            "2.0 3.6716461181640625e-05\n",
            "2.0 3.600120544433594e-05\n",
            "2.0 3.528594970703125e-05\n",
            "2.0 3.457069396972656e-05\n",
            "2.0 3.3855438232421875e-05\n",
            "2.0 3.314018249511719e-05\n",
            "2.0 3.24249267578125e-05\n",
            "2.0 3.170967102050781e-05\n",
            "2.0 3.0994415283203125e-05\n",
            "2.0 3.0279159545898438e-05\n",
            "2.0 2.956390380859375e-05\n",
            "2.0 2.9087066650390625e-05\n",
            "2.0 2.86102294921875e-05\n",
            "2.0 2.8133392333984375e-05\n",
            "2.0 2.765655517578125e-05\n",
            "2.0 2.7179718017578125e-05\n",
            "2.0 2.6702880859375e-05\n",
            "2.0 2.6226043701171875e-05\n",
            "2.0 2.574920654296875e-05\n",
            "2.0 2.5272369384765625e-05\n",
            "2.0 2.47955322265625e-05\n",
            "2.0 2.4318695068359375e-05\n",
            "2.0 2.384185791015625e-05\n",
            "2.0 2.3365020751953125e-05\n",
            "2.0 2.288818359375e-05\n",
            "2.0 2.2411346435546875e-05\n",
            "2.0 2.193450927734375e-05\n",
            "2.0 2.1457672119140625e-05\n",
            "2.0 2.09808349609375e-05\n",
            "2.0 2.0503997802734375e-05\n",
            "2.0 2.002716064453125e-05\n",
            "2.0 1.9550323486328125e-05\n",
            "2.0 1.9073486328125e-05\n",
            "2.0 1.8596649169921875e-05\n",
            "2.0 1.811981201171875e-05\n",
            "2.0 1.7642974853515625e-05\n",
            "2.0 1.7404556274414062e-05\n",
            "2.0 1.71661376953125e-05\n",
            "2.0 1.6927719116210938e-05\n",
            "2.0 1.6689300537109375e-05\n",
            "2.0 1.6450881958007812e-05\n",
            "2.0 1.621246337890625e-05\n",
            "2.0 1.5974044799804688e-05\n",
            "2.0 1.5735626220703125e-05\n",
            "2.0 1.5497207641601562e-05\n",
            "2.0 1.52587890625e-05\n",
            "2.0 1.5020370483398438e-05\n",
            "2.0 1.4781951904296875e-05\n",
            "2.0 1.4543533325195312e-05\n",
            "2.0 1.430511474609375e-05\n",
            "2.0 1.4066696166992188e-05\n",
            "2.0 1.3828277587890625e-05\n",
            "2.0 1.3589859008789062e-05\n",
            "2.0 1.33514404296875e-05\n",
            "2.0 1.3113021850585938e-05\n",
            "2.0 1.2874603271484375e-05\n",
            "2.0 1.2636184692382812e-05\n",
            "2.0 1.239776611328125e-05\n",
            "2.0 1.2159347534179688e-05\n",
            "2.0 1.1920928955078125e-05\n",
            "2.0 1.1682510375976562e-05\n",
            "2.0 1.1444091796875e-05\n",
            "2.0 1.1205673217773438e-05\n",
            "2.0 1.0967254638671875e-05\n",
            "2.0 1.0728836059570312e-05\n",
            "2.0 1.049041748046875e-05\n",
            "2.0 1.0251998901367188e-05\n",
            "2.0 1.0013580322265625e-05\n",
            "2.0 9.775161743164062e-06\n",
            "2.0 9.5367431640625e-06\n",
            "2.0 9.298324584960938e-06\n",
            "2.0 9.059906005859375e-06\n",
            "2.0 8.821487426757812e-06\n",
            "2.0 8.58306884765625e-06\n",
            "2.0 8.344650268554688e-06\n",
            "2.0 8.106231689453125e-06\n",
            "2.0 7.867813110351562e-06\n",
            "2.0 7.62939453125e-06\n",
            "2.0 7.3909759521484375e-06\n",
            "2.0 7.152557373046875e-06\n",
            "2.0 6.9141387939453125e-06\n",
            "2.0 6.67572021484375e-06\n",
            "2.0 6.4373016357421875e-06\n",
            "2.0 6.198883056640625e-06\n",
            "2.0 5.9604644775390625e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "2.0 5.7220458984375e-06\n",
            "y is minimum at x = 1\n",
            "x =  1.0000028610229492\n",
            "y =  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzfnq7qbnH4t",
        "colab_type": "text"
      },
      "source": [
        "### Modules (layers)\n",
        "\n",
        "`Module` is a basic component of a computation graph in PyTorch, including all kinds of layers (https://pytorch.org/docs/stable/nn.html). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAp_UpbMnHcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e5cf9acc-1f21-4bd2-dbf3-cf33fae1f027"
      },
      "source": [
        "# Layers\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "input_dim = 20\n",
        "output_dim = 30\n",
        "\n",
        "torch.manual_seed(123)\n",
        "input = torch.randn(10, input_dim)\n",
        "\n",
        "# Linear transform (fully connected layer)\n",
        "linear_module = nn.Linear(input_dim, output_dim, bias=True)\n",
        "output = linear_module(input)\n",
        "\n",
        "print('After linear', output.shape)\n",
        "\n",
        "# ReLU\n",
        "relu_module = nn.ReLU()\n",
        "relu_output = relu_module(output)\n",
        "\n",
        "print('Before ReLU:', output[0, :5])\n",
        "print('Chaining modules:', relu_output[0, :5])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After linear torch.Size([10, 30])\n",
            "Before ReLU: tensor([ 0.3038,  0.5615, -0.4189, -0.0026,  0.0013], grad_fn=<SliceBackward>)\n",
            "Chaining modules: tensor([0.3038, 0.5615, 0.0000, 0.0000, 0.0013], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W71Xl5MOj73H",
        "colab_type": "text"
      },
      "source": [
        "### Sequential (sequence of modules)\n",
        "\n",
        "`nn.Sequential(module1, module2, ...)` can chain a sequence of modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1syBPOxYkGLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f1e20e9-51f5-4238-ffdc-9d6567ef1d13"
      },
      "source": [
        "# Combine modules\n",
        "sequential_module = nn.Sequential(linear_module, relu_module)\n",
        "\n",
        "sequential_output = sequential_module(input)\n",
        "\n",
        "print('Sequential:', sequential_output[0, :5])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential: tensor([0.3038, 0.5615, 0.0000, 0.0000, 0.0013], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnWq2sCIpLJY",
        "colab_type": "text"
      },
      "source": [
        "### Models\n",
        "\n",
        "To build complex modules and reuse them, you can define your own Module class, like `Model1`, `Model2`, and `Model3`.\n",
        "Modules (layers) defined in a module will be automatically added to the computation graph.\n",
        "\n",
        "Your custom module should inherit from `nn.Module`. `super().__init__()` should be added in your `__init__()` method.\n",
        "You also need to implement your own `forward()` function which describes the forward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuYvIGJgDH7r",
        "colab_type": "text"
      },
      "source": [
        "Define layers using member variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cory4c0ypO5v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "41e99b23-9323-4b17-9ca8-cbf9ed35691c"
      },
      "source": [
        "class Model1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() # must be added before define you module\n",
        "    \n",
        "    # Define layers as member variables\n",
        "    self.linear_module = nn.Linear(input_dim, output_dim, bias=True)\n",
        "    self.relu = nn.ReLU()\n",
        "    \n",
        "  def forward(self, input):\n",
        "    # Define forward pass (input -> linear module -> relu -> output)\n",
        "    output1 = self.linear_module(input)\n",
        "    output2 = self.relu(output1)\n",
        "    return output2\n",
        "\n",
        "  \n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Make a model\n",
        "model1 = Model1()\n",
        "\n",
        "# Run forward pass. model1(input) == model1.forward(input)\n",
        "model1_output = model1(input)\n",
        "\n",
        "print(model1)\n",
        "print('Model 1 output:', model1_output[0, :5])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model1(\n",
            "  (linear_module): Linear(in_features=20, out_features=30, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n",
            "Model 1 output: tensor([0.0000, 0.4561, 0.0000, 0.0000, 0.0000], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiwedfslDPpk",
        "colab_type": "text"
      },
      "source": [
        "Define layers using `nn.Sequential`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8321V5bjWM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "17f40d84-7316-42d6-d629-810b01c88f62"
      },
      "source": [
        "class Model2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Define layers as a sequential\n",
        "    \n",
        "    linear = nn.Linear(input_dim, output_dim,bias=True)\n",
        "    relu = nn.ReLU()\n",
        "#     fc = []\n",
        "#     for i in range(10):\n",
        "#       fc.append(nn.)\n",
        "\n",
        "    self.seq = nn.Sequential(linear, relu)\n",
        "      \n",
        "      \n",
        "    pass\n",
        "    \n",
        "  def forward(self, input):\n",
        "    output = self.seq (input)\n",
        "    return output\n",
        "  \n",
        "  \n",
        "torch.manual_seed(123)\n",
        "model2 = Model2()\n",
        "model2_output = model2(input)\n",
        "\n",
        "print(model2)\n",
        "print('Model 2 output:', model2_output[0, :5])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model2(\n",
            "  (seq): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            ")\n",
            "Model 2 output: tensor([0.0000, 0.4561, 0.0000, 0.0000, 0.0000], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diqlp6CUDTF0",
        "colab_type": "text"
      },
      "source": [
        "Define layers using `nn.ModuleList`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2r77JGC1jZKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "74a3dec5-7292-484a-8e24-7e022c98e67e"
      },
      "source": [
        "class Model3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Define layers as a module list\n",
        "    linear = nn.Linear(input_dim, output_dim,bias=True)\n",
        "    relu = nn.ReLU()\n",
        "    \n",
        "    self.module_list = nn.ModuleList([linear, relu])\n",
        "    pass\n",
        "    \n",
        "  def forward(self, input):\n",
        "    \n",
        "    output1 = self.module_list[0](input)\n",
        "    output2 = self.module_list[1](output1)\n",
        "    return output2\n",
        "  \n",
        "  \n",
        "torch.manual_seed(123)\n",
        "model3 = Model3()\n",
        "model3_output = model3(input)\n",
        "\n",
        "print(model3)\n",
        "print('Model 3 output:', model3_output[0, :5])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model3(\n",
            "  (module_list): ModuleList(\n",
            "    (0): Linear(in_features=20, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            ")\n",
            "Model 3 output: tensor([0.0000, 0.4561, 0.0000, 0.0000, 0.0000], grad_fn=<SliceBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOYTp4d6lh2h",
        "colab_type": "text"
      },
      "source": [
        "### Simple MLP module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9hI4u88lbV2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "181adead-03ab-4fbc-945e-1c879d525190"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dims=[]):\n",
        "    super().__init__()\n",
        "    \n",
        "    # Define layers\n",
        "    fc = []\n",
        "    previous_dim = input_dim\n",
        "    for dim in hidden_dims:\n",
        "      fc.append(\n",
        "          nn.Linear(previous_dim, dim) \n",
        "      )\n",
        "      fc.append(\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      previous_dim = dim\n",
        "    \n",
        "    fc.append(\n",
        "        nn.Linear(previous_dim, output_dim)\n",
        "    )\n",
        "    \n",
        "    # Convert a list of layers to a sequential module\n",
        "    self.fc = nn.Sequential(*fc)\n",
        "  \n",
        "  def forward(self, observation):\n",
        "    return self.fc(observation)\n",
        "  \n",
        "  \n",
        "mlp = MLP(100, 20, [30, 30,40,50])\n",
        "print(mlp)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=30, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=30, out_features=30, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=30, out_features=40, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=40, out_features=50, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=50, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFxguuPOlkso",
        "colab_type": "text"
      },
      "source": [
        "### Be careful!\n",
        "\n",
        "If layers are not referenced by member variables and not added to `Sequential`, `ModuleList`, and `ModuleDict`, your module cannot update those layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POEGEaoZlnT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f75325a-7a2a-4833-a340-d01c881bc4c1"
      },
      "source": [
        "# Wrong MLP module\n",
        "class MLP_wrong(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim, hidden_dims=[]):\n",
        "    super().__init__()\n",
        "    \n",
        "    fc = []\n",
        "    previous_dim = input_dim\n",
        "    for dim in hidden_dims:\n",
        "      fc.append(\n",
        "          nn.Linear(previous_dim, dim)\n",
        "      )\n",
        "      fc.append(\n",
        "          nn.ReLU()\n",
        "      )\n",
        "      previous_dim = dim\n",
        "    \n",
        "    fc.append(\n",
        "        nn.Linear(previous_dim, output_dim)\n",
        "    )\n",
        "    \n",
        "    self.fc = fc # MLP module does not know about these layers\n",
        "    \n",
        "  def forward(self, observation):\n",
        "    output = observation\n",
        "    for layer in self.fc:\n",
        "      output = self.fc(output)\n",
        "    return output\n",
        "\n",
        "  \n",
        "mlp_wrong = MLP_wrong(100, 20, [30, 30])\n",
        "print(mlp_wrong)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP_wrong()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0T_QNdipHq9",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ZtNkQKhQiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7c68d9e1-3d6f-43ee-d30f-320756bc8650"
      },
      "source": [
        "# Optimizers\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model2.parameters(), lr=1e-3)\n",
        "\n",
        "true_output = torch.rand(output_dim)\n",
        "rnd_input = torch.rand(input_dim)\n",
        "\n",
        "for _ in range(10):\n",
        "  # Predicted output\n",
        "  predicted_output = model2(rnd_input)\n",
        "  \n",
        "  # Compute loss\n",
        "  loss = loss_fn(predicted_output, true_output)\n",
        "  print(loss)\n",
        "\n",
        "  # Update model\n",
        "  optimizer.zero_grad() # Zero out previous gradients\n",
        "  loss.backward()  # Compute new gradients\n",
        "  optimizer.step()  # Update parameters"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.2142, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2103, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2066, grad_fn=<MseLossBackward>)\n",
            "tensor(0.2031, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1997, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1964, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1932, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1901, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1872, grad_fn=<MseLossBackward>)\n",
            "tensor(0.1844, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhZ3sZgiPUI",
        "colab_type": "text"
      },
      "source": [
        "### GPU support\n",
        "\n",
        "You need to define your tensors in GPU.\n",
        "\n",
        "It can be done by \n",
        "\n",
        "`x = torch.tensor(data, device=torch.device(\"cuda\"))` \n",
        "\n",
        "or \n",
        "\n",
        "`x = torch.tensor(data).to(torch.device(\"cuda\"))`.\n",
        "\n",
        "or \n",
        "\n",
        "`x = torch.tensor(data).cuda()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmnclDosiR9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cpu\") \n",
        "# For gpu\n",
        "# device = torch.device(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDV-ghZJRm4T",
        "colab_type": "text"
      },
      "source": [
        "## OpenAI gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFLNQXMWP0mf",
        "colab_type": "text"
      },
      "source": [
        "### Installation\n",
        "\n",
        "You can simply install OpenAI gym by executing `pip install gym`.\n",
        "\n",
        "Here, we are using a virtual display to render videos in a headless server. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnEOmhbeuL42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "45cedc4f-d9d7-420f-c40a-40df96f5b229"
      },
      "source": [
        "# Render OpenAI gym: code from https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t\n",
        "\n",
        "!pip install gym > /dev/null 2>&1\n",
        "!apt-get install python-opengl -y > /dev/null 2>&1\n",
        "!apt install xvfb -y > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install pyglet==1.3.2 > /dev/null 2>&1\n",
        "\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "\n",
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TInxwg9Wyex6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from collections import defaultdict\n",
        "\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "import numpy as np\n",
        "np.set_printoptions(precision=2, suppress=True)\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cx1kmBfQVbo",
        "colab_type": "text"
      },
      "source": [
        "### Make an environment!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtV4hhHdy7Bt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "e05ddfb5-102f-4b65-a6a1-aa495c33c819"
      },
      "source": [
        "# Make an environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# Observation space\n",
        "print('Observation space:', env.observation_space)\n",
        "\n",
        "# Action space\n",
        "print('Action space:', env.action_space)\n",
        "\n",
        "# Reset an episode and get an initial state\n",
        "observation = env.reset()\n",
        "\n",
        "print('Initial observation:', observation)\n",
        "\n",
        "# Render an image\n",
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(4,)\n",
            "Action space: Discrete(2)\n",
            "Initial observation: [ 0.03  0.01 -0.01 -0.05]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ffb25a73320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEBCAYAAACUmXXrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n92kY+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXqAaAx3pyyx",
        "colab_type": "text"
      },
      "source": [
        "### Take a step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR2CQcCNQ1Fc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "6725c1e5-cbc2-4836-c013-eee2345de024"
      },
      "source": [
        "# Sample one random action\n",
        "random_action = env.action_space.sample()\n",
        "print('Random action', random_action)\n",
        "\n",
        "# Take a step\n",
        "observation_next, reward, done, info = env.step(random_action)\n",
        "\n",
        "print('Observation', observation)\n",
        "print('Next observation', observation_next)\n",
        "print('Reward', reward)\n",
        "print('Done', done)\n",
        "print('Info', info)\n",
        "\n",
        "# Render an image\n",
        "plt.imshow(env.render('rgb_array'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random action 0\n",
            "Observation [ 0.03  0.01 -0.01 -0.05]\n",
            "Next observation [ 0.03 -0.19 -0.01  0.24]\n",
            "Reward 1.0\n",
            "Done False\n",
            "Info {}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ffb23189710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEBCAYAAACUmXXrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n92kY+gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrrmm_yAqF-4",
        "colab_type": "text"
      },
      "source": [
        "### Run one episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqAoppv-RGL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "c1891980-e5a6-4a28-9a68-7ac5d05ce8bb"
      },
      "source": [
        "# Make a new environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# Wrap environment for recording\n",
        "env = wrap_env(env)\n",
        "\n",
        "# Initialize environment\n",
        "observation = env.reset()\n",
        "\n",
        "done = False\n",
        "episode_reward = 0\n",
        "episode_length = 0\n",
        "\n",
        "# Run until done == True\n",
        "while not done:\n",
        "  # Take a step\n",
        "  observation, reward, done, info = env.step(env.action_space.sample())\n",
        "  \n",
        "  episode_reward += reward\n",
        "  episode_length += 1\n",
        "  \n",
        "print('Total reward:', episode_reward)\n",
        "print('Total length:', episode_length)\n",
        "\n",
        "env.close()\n",
        "show_video()  "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total reward: 11.0\n",
            "Total length: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkyoyKkQRr3S",
        "colab_type": "text"
      },
      "source": [
        "## Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-VXdfNV_y5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataset for Q-learning\n",
        "# Also called \"experience replay buffer\", \"replay memory\"\n",
        "class Dataset(object):\n",
        "  \n",
        "  def __init__(self, size):\n",
        "    self._size = size\n",
        "    self._transitions = []\n",
        "    self._index = 0\n",
        "    \n",
        "  # Store a transition (s_t, a_t, s_t+1, r_t, done)\n",
        "  def store(self, obs, action, obs_next, reward, done):\n",
        "    transition = {'obs': obs, 'action': action, \n",
        "                  'obs_next': obs_next, 'reward': reward, 'done': done}\n",
        "    \n",
        "    if self._index < self._size:\n",
        "      self._transitions.append(transition)\n",
        "    else:\n",
        "      self._transitions[self._index] = transition\n",
        "      \n",
        "    self._index = (self._index + 1) % self._size\n",
        "\n",
        "  # Sample `batch_size` transitions from the dataset and convert to tensors\n",
        "  def sample(self, batch_size):\n",
        "    indexes = np.random.randint(0, len(self._transitions), batch_size)\n",
        "    batch = {\n",
        "        k: np.stack([self._transitions[i][k] for i in indexes])\n",
        "        for k in ['obs', 'action', 'obs_next', 'reward', 'done']}\n",
        "    return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z537CR_mmW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Logger(object):\n",
        "  \n",
        "  def __init__(self):\n",
        "    self._history = defaultdict(list)\n",
        "    self._global_history = defaultdict(list)\n",
        "  \n",
        "  def add(self, info):\n",
        "    for k, v in info.items():\n",
        "      self._history[k].append(v)\n",
        "      self._global_history[k].append(v)\n",
        "      \n",
        "  def summary(self):\n",
        "    summary = []\n",
        "    for k, v in self._history.items():\n",
        "      summary.append('{}: {:.2f}'.format(k, np.mean(v)))\n",
        "      \n",
        "    print(', '.join(summary))\n",
        "      \n",
        "    self._history = defaultdict(list)\n",
        "    \n",
        "  def plot(self, key):\n",
        "    n = len(self._global_history[key])\n",
        "    plt.plot(np.arange(n), self._global_history[key])\n",
        "    plt.title(key)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzOw9o0u_zqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Q-network, an approximation of a Q-value function, Q(s, a)\n",
        "class QNetwork(nn.Module):\n",
        "  \n",
        "  def __init__(self, observation_space, action_space):\n",
        "    super().__init__()\n",
        "\n",
        "    self._action_space = action_space\n",
        "    self._eps = 1\n",
        "\n",
        "    # Input: observation\n",
        "    # Output: q-value for each action\n",
        "    self.fc = MLP(observation_space.shape[0], action_space.n, [32, 32])\n",
        "    print(self.fc)\n",
        "    \n",
        "  def forward(self, observation):\n",
        "    q_values = self.fc(observation)\n",
        "    return q_values\n",
        "  \n",
        "  def act(self, observation, is_train=True):\n",
        "    # Epsilon greedy for exploration\n",
        "    if is_train and np.random.uniform() < self._eps:\n",
        "      action = self._action_space.sample()\n",
        "    else:\n",
        "      # Observation is numpy ndarray\n",
        "      # Convert numpy array to tensor\n",
        "      obs = torch.tensor([observation]).to(device, dtype=torch.float32)\n",
        "      \n",
        "      # Compute q-values\n",
        "      q_values = self.forward(obs)\n",
        "      q_values = q_values.detach().cpu().numpy().squeeze()\n",
        "      \n",
        "      # Take an action with maximum value\n",
        "      action = np.argmax(q_values)\n",
        "    \n",
        "    return action\n",
        "  \n",
        "  def epsilon_decay(self):\n",
        "    self._eps -= 0.01\n",
        "    self._eps = max(self._eps, 0.01)\n",
        "    \n",
        "  def state_dict(self):\n",
        "    state_dict = super().state_dict()\n",
        "    state_dict['eps'] = self._eps\n",
        "    return state_dict\n",
        "  \n",
        "  def load_state_dict(self, state_dict):\n",
        "    self._eps = state_dict['eps']\n",
        "    state_dict.pop('eps')\n",
        "    super().load_state_dict(state_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ijX2c6iOXME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer(object):\n",
        "  def __init__(self, env, model, dataset):\n",
        "    self._env = env\n",
        "    self._model = model\n",
        "    self._dataset = dataset\n",
        "    self._logger = Logger()\n",
        "    \n",
        "    self._optim = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "  def run_one_episode(self, env, is_train=True):\n",
        "    model = self._model\n",
        "    dataset = self._dataset\n",
        "    \n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    episode_length = 0\n",
        "    episode_reward = 0\n",
        "    \n",
        "    while not done:\n",
        "      \n",
        "      # Sample an action\n",
        "      action = model.act(observation, is_train)\n",
        "      \n",
        "      # Take a step\n",
        "      observation_next, reward, done, info = env.step(action)\n",
        "      \n",
        "      # Add a transition to the dataset\n",
        "      dataset.store(observation, action, observation_next, reward, done)\n",
        "      observation = observation_next\n",
        "      \n",
        "      episode_length += 1\n",
        "      episode_reward += reward\n",
        "      \n",
        "      # Train q-network\n",
        "      if is_train:\n",
        "        batch = dataset.sample(batch_size=20)\n",
        "        loss = self._update(batch)\n",
        "        model.epsilon_decay()\n",
        "        \n",
        "        self._logger.add({'loss': loss})\n",
        "        self._logger.add({'action': action})\n",
        "      \n",
        "    if is_train:\n",
        "      self._logger.add({'length': episode_length, 'reward': episode_reward})  \n",
        "    else:\n",
        "      print('reward: {}'.format(episode_reward))\n",
        "          \n",
        "  def _update(self, batch):\n",
        "    batch_size = len(batch['obs'])\n",
        "    \n",
        "    obs = torch.tensor(batch['obs'], dtype=torch.float32)\n",
        "    obs_next = torch.tensor(batch['obs_next'], dtype=torch.float32)\n",
        "    rew = torch.tensor(batch['reward'], dtype=torch.float32).reshape(batch_size, 1)\n",
        "    ac = torch.eye(self._env.action_space.n)[batch['action']]\n",
        "    done = torch.tensor(batch['done'], dtype=torch.float32).reshape(batch_size, 1)\n",
        "    \n",
        "    discount_factor = 0.95\n",
        "    \n",
        "    # y\n",
        "    with torch.no_grad():\n",
        "      q_next = self._model(obs_next).max(dim=1, keepdim=True)[0]\n",
        "      \n",
        "      q_target = rew + (1 - done) * discount_factor * q_next\n",
        "    \n",
        "    # Q(s,a)\n",
        "    q_predict = torch.sum(self._model(obs) * ac, dim=1, keepdim=True)    \n",
        "    \n",
        "    # Mean square error\n",
        "    loss = (q_predict - q_target).pow(2).mean()\n",
        "    \n",
        "    # Backpropagation\n",
        "    self._optim.zero_grad()\n",
        "    loss.backward()\n",
        "    self._optim.step()\n",
        "    \n",
        "    return loss.item()\n",
        "    \n",
        "  def train(self):\n",
        "    num_episode = 200\n",
        "    for i in range(num_episode):\n",
        "      self.run_one_episode(env=self._env, is_train=True)\n",
        "      \n",
        "      if (i + 1) % 10 == 0:\n",
        "        print('training episode {}/{}'.format(i + 1, num_episode))\n",
        "        self._logger.summary()\n",
        "\n",
        "  def evaluate(self):\n",
        "    env_test = wrap_env(gym.make('CartPole-v0'))\n",
        "    for i in range(10):\n",
        "      self.run_one_episode(env=env_test, is_train=False)\n",
        "    env_test.close()\n",
        "    show_video()\n",
        "    \n",
        "  def plot(self, key):\n",
        "    self._logger.plot(key)\n",
        "    \n",
        "  def state_dict(self):\n",
        "    return {'optim': self._optim.state_dict()}\n",
        "  \n",
        "  def load_state_dict(self, state_dict):\n",
        "    self._optim.load_state_dict(state_dict['optim'])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCR4K6sfM262",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0872cf64-0ff1-421b-969f-55b36d9bf157"
      },
      "source": [
        "# Make sure setting up random seeds before training\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ffb3c00acb0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URKNQPUjSaSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "9a2b07d0-36ed-4159-9173-f4d91a66d590"
      },
      "source": [
        "# Environment\n",
        "env = gym.make('CartPole-v0')\n",
        "env.seed(123)\n",
        "\n",
        "# Model\n",
        "policy = QNetwork(env.observation_space, env.action_space)\n",
        "print(policy)\n",
        "\n",
        "# Dataset\n",
        "dataset = Dataset(10000)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "QNetwork(\n",
            "  (fc): MLP(\n",
            "    (fc): Sequential(\n",
            "      (0): Linear(in_features=4, out_features=32, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
            "      (3): ReLU()\n",
            "      (4): Linear(in_features=32, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmZy2tyd0tkA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "outputId": "84724c45-0fb4-423b-c8f9-c25e04d8efad"
      },
      "source": [
        "# Trainer\n",
        "trainer = Trainer(env, policy, dataset)\n",
        "\n",
        "# Train Q-Network\n",
        "policy.train()\n",
        "trainer.train()\n",
        "\n",
        "trainer.plot('reward')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training episode 10/200\n",
            "loss: 1.61, action: 0.79, length: 11.50, reward: 11.50\n",
            "training episode 20/200\n",
            "loss: 6.41, action: 0.96, length: 9.50, reward: 9.50\n",
            "training episode 30/200\n",
            "loss: 3.39, action: 0.71, length: 9.50, reward: 9.50\n",
            "training episode 40/200\n",
            "loss: 11.63, action: 0.00, length: 9.20, reward: 9.20\n",
            "training episode 50/200\n",
            "loss: 5.42, action: 0.46, length: 10.20, reward: 10.20\n",
            "training episode 60/200\n",
            "loss: 1.96, action: 0.63, length: 12.00, reward: 12.00\n",
            "training episode 70/200\n",
            "loss: 1.76, action: 0.59, length: 12.60, reward: 12.60\n",
            "training episode 80/200\n",
            "loss: 2.64, action: 0.49, length: 28.60, reward: 28.60\n",
            "training episode 90/200\n",
            "loss: 3.04, action: 0.50, length: 48.60, reward: 48.60\n",
            "training episode 100/200\n",
            "loss: 3.00, action: 0.50, length: 43.90, reward: 43.90\n",
            "training episode 110/200\n",
            "loss: 3.17, action: 0.51, length: 56.90, reward: 56.90\n",
            "training episode 120/200\n",
            "loss: 3.01, action: 0.50, length: 47.80, reward: 47.80\n",
            "training episode 130/200\n",
            "loss: 2.16, action: 0.52, length: 132.10, reward: 132.10\n",
            "training episode 140/200\n",
            "loss: 2.07, action: 0.51, length: 199.00, reward: 199.00\n",
            "training episode 150/200\n",
            "loss: 2.09, action: 0.50, length: 196.00, reward: 196.00\n",
            "training episode 160/200\n",
            "loss: 2.12, action: 0.50, length: 200.00, reward: 200.00\n",
            "training episode 170/200\n",
            "loss: 1.93, action: 0.51, length: 199.60, reward: 199.60\n",
            "training episode 180/200\n",
            "loss: 1.90, action: 0.51, length: 200.00, reward: 200.00\n",
            "training episode 190/200\n",
            "loss: 1.62, action: 0.51, length: 199.70, reward: 199.70\n",
            "training episode 200/200\n",
            "loss: 1.71, action: 0.51, length: 200.00, reward: 200.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAELCAYAAAA/cjqaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl5FrWb5BrFQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clm_5tH91QJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save checkpoint\n",
        "os.makedirs('ckpt', exist_ok=True)\n",
        "ckpt_path = os.path.join('ckpt', 'ckpt.pt')\n",
        "\n",
        "# Get parameters from models and optimizers\n",
        "state_dict = {}\n",
        "state_dict['trainer'] = trainer.state_dict()\n",
        "state_dict['policy'] = policy.state_dict()\n",
        "\n",
        "torch.save(state_dict, ckpt_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btv-MRUx1dmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load checkpoint\n",
        "ckpt = torch.load(ckpt_path)\n",
        "\n",
        "# Update parameters with checkpoint\n",
        "trainer.load_state_dict(ckpt['trainer'])\n",
        "policy.load_state_dict(ckpt['policy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruzW9OIvJ6UO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "aa6714ef-0ab5-4a82-a67d-1c2ce91550b7"
      },
      "source": [
        "# Run evaluation\n",
        "policy.eval()\n",
        "trainer.evaluate()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reward: 200.0\n",
            "reward: 200.0\n",
            "reward: 200.0\n",
            "reward: 200.0\n",
            "reward: 200.0\n",
            "reward: 200.0\n",
            "reward: 200.0\n",
            "reward: 200.0\n",
            "reward: 193.0\n",
            "reward: 200.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}